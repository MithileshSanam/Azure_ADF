{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "773602bc-8442-4bdd-ba1c-54b19f510896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./CommonUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2795001-bccf-40be-94c0-f14351abaab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./CreateOrReplaceTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60f7f480-fa0c-4f80-bdd5-15b8abe0097e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faea656f-8152-4acf-bcc5-ede696ea2902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SCD1:\n",
    "    def __init__(self,entity_name,pk,ingestion_layer):\n",
    "        self.entity_name = entity_name.capitalize()\n",
    "        self.pk = pk.split(\",\")\n",
    "        self.ingestion_layer = ingestion_layer\n",
    "\n",
    "    def create_src_view(self):\n",
    "        src_df = spark.read.parquet(f\"{get_mnt('landing')}/{self.entity_name}/{self.entity_name}.parquet\")\n",
    "        src_df = src_df.withColumn(\"processing_dttm\",current_timestamp())\n",
    "        src_df.registerTempTable(\"source_tbl\")\n",
    "        return src_df,'source_tbl'\n",
    "    \n",
    "    def check_or_create_table(self,src_df):\n",
    "        table_count = spark.sql(f\"show tables in {get_catalog_name(self.entity_name,self.ingestion_layer)}\").select('tableName').filter(f\"tableName = '{self.entity_name.lower().split('_inc')[0]}'\").count()\n",
    "        print(table_count)\n",
    "        if table_count ==0:\n",
    "            print(\"Table doesnt exist. Creating schema\")\n",
    "            crt_execute(self.entity_name,self.ingestion_layer,truncate_flag=True,src_df=src_df)\n",
    "\n",
    "        target_tbl = f\"hive_metastore.{get_catalog_name(self.entity_name,self.ingestion_layer)}.{self.entity_name.split('_inc')[0]}\"\n",
    "        return target_tbl\n",
    "    \n",
    "    def execute_scd_1(self):\n",
    "\n",
    "        src_df,source_tbl = self.create_src_view()\n",
    "        target_tbl = self.check_or_create_table(src_df)\n",
    "        pk_cond = \" AND \".join([f\"target.{pk_col} = source.{pk_col}\" for pk_col in self.pk])\n",
    "        update_cond = \" , \".join([f\"target.{u_col} = source.{u_col}\" for u_col in src_df.columns])\n",
    "        insert_cols= \", \".join(src_df.columns)\n",
    "        insert_values= \", \".join([f\"source.{icol}\" for icol in src_df.columns])\n",
    "        query = f'''\n",
    "        MERGE INTO {target_tbl} AS target\n",
    "        USING {source_tbl} AS source\n",
    "        ON {pk_cond}\n",
    "        WHEN MATCHED\n",
    "            THEN \n",
    "                UPDATE SET \n",
    "                    {update_cond}\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT ({insert_cols})\n",
    "            VALUES ({insert_values});\n",
    "\n",
    "        '''\n",
    "\n",
    "        spark.sql(query).display()\n",
    "# SCD1('Arancione_incremental','ArancioneId',ingestion_layer='bronze').execute_scd_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aec6dca-5d54-45f4-ae07-89b1b846ea3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SCD2:\n",
    "    def __init__(self,entity_name,pk,ingestion_layer):\n",
    "        self.entity_name = entity_name.capitalize()\n",
    "        self.pk = pk.split(\",\")\n",
    "        self.ingestion_layer = ingestion_layer\n",
    "\n",
    "    def table_check_count(self,ingestion_layer):\n",
    "        table_count = spark.sql(f\"show tables in {get_catalog_name(self.entity_name,ingestion_layer)}\").select('tableName').filter(f\"tableName = '{self.entity_name.lower().split('_inc')[0]}'\").count()\n",
    "        return table_count\n",
    "\n",
    "    def create_src_view(self):\n",
    "        src_df = spark.read.table(f'hive_metastore.bronze_incremental_schema.{self.entity_name.lower().split('_inc')[0]}'),withColumnRenamed('lastUpdateDate','sourceLastUpdateDate')\n",
    "        if self.table_check_count('silver') != 0:\n",
    "            max_ts = src_df.selectExpr('max(processing_dttm)').collect()[0][0]\n",
    "            src_df = src_df.filter(f'processing_dttm = \"{max_ts}\"')\n",
    "        src_df = src_df.withColumn(\"start_date\",lit(None).cast(TimestampType()))\n",
    "        src_df = src_df.withColumn(\"end_date\",lit(None).cast(TimestampType()))\n",
    "        src_df = src_df.withColumn(\"processing_dttm\",current_timestamp())\n",
    "        src_df.registerTempTable(\"source_tbl\")\n",
    "        return src_df,'source_tbl'\n",
    "    \n",
    "    def check_or_create_table(self,src_df):\n",
    "        if self.table_check_count('silver') == 0:\n",
    "            print(\"Table doesnt exist. Creating schema\")\n",
    "            crt_execute(self.entity_name,self.ingestion_layer,truncate_flag=True,src_df=src_df)\n",
    "\n",
    "        target_tbl = f\"hive_metastore.{get_catalog_name(self.entity_name,self.ingestion_layer)}.{self.entity_name.split('_inc')[0]}\"\n",
    "        return target_tbl\n",
    "    \n",
    "    def execute_scd_2(self):\n",
    "\n",
    "        src_df,source_tbl = self.create_src_view()\n",
    "        # print(\"=======printing target before merge======\")\n",
    "        # display(spark.read.table(f\"hive_metastore.{get_catalog_name(self.entity_name,self.ingestion_layer)}.{self.entity_name.split('_inc')[0]}\"))\n",
    "        target_tbl = self.check_or_create_table(src_df)\n",
    "        pk_cond = \" AND \".join([f\"target.{pk_col} = source.{pk_col}\" for pk_col in self.pk])\n",
    "        update_cond = \" , \".join([f\"target.{u_col} = source.{u_col}\" for u_col in src_df.columns])\n",
    "        scd2_cond = \" OR \".join([f\"target.{u_col} != source.{u_col}\" for u_col in src_df.columns if u_col not in ['processing_dttm']+self.pk])\n",
    "        # print(update_cond)\n",
    "        insert_cols= \", \".join(src_df.columns)\n",
    "        insert_values= \", \".join([f\"source.{icol}\" for icol in src_df.columns])\n",
    "        query = f'''\n",
    "        MERGE INTO {target_tbl} AS target\n",
    "        USING {source_tbl} AS source\n",
    "        ON {pk_cond}\n",
    "        WHEN MATCHED AND ({scd2_cond})\n",
    "            THEN \n",
    "                UPDATE SET \n",
    "                    {update_cond}\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT ({insert_cols})\n",
    "            VALUES ({insert_values});\n",
    "\n",
    "        '''\n",
    "\n",
    "        spark.sql(query).display()\n",
    "        # display(spark.read.table(target_tbl))\n",
    "\n",
    "# SCD2('Celeste_incremental','TransactionId',ingestion_layer='silver').execute_scd_2()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6812213841852458,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD_Strategy",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
