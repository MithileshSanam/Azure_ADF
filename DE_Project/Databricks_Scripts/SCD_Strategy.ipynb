{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "773602bc-8442-4bdd-ba1c-54b19f510896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./CommonUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2795001-bccf-40be-94c0-f14351abaab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./CreateOrReplaceTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60f7f480-fa0c-4f80-bdd5-15b8abe0097e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faea656f-8152-4acf-bcc5-ede696ea2902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SCD1:\n",
    "    def __init__(self,entity_name,pk,ingestion_layer):\n",
    "        self.entity_name = entity_name.capitalize()\n",
    "        self.pk = pk.split(\",\")\n",
    "        self.ingestion_layer = ingestion_layer\n",
    "\n",
    "    def create_src_view(self):\n",
    "        src_df = spark.read.parquet(f\"{get_mnt('landing')}/{self.entity_name}/{self.entity_name}.parquet\")\n",
    "        src_df = src_df.withColumn(\"processing_dttm\",current_timestamp())\n",
    "        src_df.registerTempTable(\"source_tbl\")\n",
    "        return src_df,'source_tbl'\n",
    "    \n",
    "    def check_or_create_table(self,src_df):\n",
    "        table_count = spark.sql(f\"show tables in {get_catalog_name(self.entity_name,self.ingestion_layer)}\").select('tableName').filter(f\"tableName = '{self.entity_name.lower().split('_inc')[0]}'\").count()\n",
    "        print(table_count)\n",
    "        if table_count ==0:\n",
    "            print(\"Table doesnt exist. Creating schema\")\n",
    "            crt_execute(self.entity_name,self.ingestion_layer,truncate_flag=True,src_df=src_df)\n",
    "\n",
    "        target_tbl = f\"hive_metastore.{get_catalog_name(self.entity_name,self.ingestion_layer)}.{self.entity_name.split('_inc')[0]}\"\n",
    "        return target_tbl\n",
    "    \n",
    "    def execute_scd_1(self):\n",
    "\n",
    "        src_df,source_tbl = self.create_src_view()\n",
    "        target_tbl = self.check_or_create_table(src_df)\n",
    "        pk_cond = \" AND \".join([f\"target.{pk_col} = source.{pk_col}\" for pk_col in self.pk])\n",
    "        update_cond = \" , \".join([f\"target.{u_col} = source.{u_col}\" for u_col in src_df.columns])\n",
    "        insert_cols= \", \".join(src_df.columns)\n",
    "        insert_values= \", \".join([f\"source.{icol}\" for icol in src_df.columns])\n",
    "        query = f'''\n",
    "        MERGE INTO {target_tbl} AS target\n",
    "        USING {source_tbl} AS source\n",
    "        ON {pk_cond}\n",
    "        WHEN MATCHED\n",
    "            THEN \n",
    "                UPDATE SET \n",
    "                    {update_cond}\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT ({insert_cols})\n",
    "            VALUES ({insert_values});\n",
    "\n",
    "        '''\n",
    "\n",
    "        spark.sql(query).display()\n",
    "# SCD1('Arancione_incremental','ArancioneId',ingestion_layer='bronze').execute_scd_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aec6dca-5d54-45f4-ae07-89b1b846ea3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SCD2:\n",
    "    def __init__(self, entity_name, pk, ingestion_layer):\n",
    "        self.entity_name = entity_name.capitalize()\n",
    "        self.pk = pk.split(\",\")\n",
    "        self.ingestion_layer = ingestion_layer\n",
    "\n",
    "    def table_check_count(self):\n",
    "        table_count = (\n",
    "            spark.sql(\n",
    "                f\"show tables in {get_catalog_name(self.entity_name,self.ingestion_layer)}\"\n",
    "            )\n",
    "            .select(\"tableName\")\n",
    "            .filter(f\"tableName = '{self.entity_name.lower().split('_inc')[0]}'\")\n",
    "            .count()\n",
    "        )\n",
    "        return table_count\n",
    "\n",
    "    def create_src_view(self):\n",
    "        src_df = spark.read.table(\n",
    "            f\"hive_metastore.bronze_incremental_schema.{self.entity_name.lower().split('_inc')[0]}\"\n",
    "        ).withColumnRenamed(\"lastUpdateDate\", \"sourceLastUpdateDate\")\n",
    "        if self.table_check_count() != 0:\n",
    "            max_ts = src_df.selectExpr(\"max(processing_dttm)\").collect()[0][0]\n",
    "            src_df = src_df.filter(f'processing_dttm = \"{max_ts}\"')\n",
    "        src_df = src_df.withColumn(\"start_date\", lit(None).cast(TimestampType()))\n",
    "        src_df = src_df.withColumn(\"end_date\", lit(None).cast(TimestampType()))\n",
    "        src_df = src_df.withColumn(\"processing_dttm\", current_timestamp())\n",
    "        src_df.registerTempTable(\"source_tbl\")\n",
    "        return src_df, \"source_tbl\"\n",
    "\n",
    "    def check_or_create_table(self, src_df):\n",
    "        if self.table_check_count() == 0:\n",
    "            print(\"Table doesnt exist. Creating schema\")\n",
    "            crt_execute(\n",
    "                self.entity_name,\n",
    "                self.ingestion_layer,\n",
    "                truncate_flag=True,\n",
    "                src_df=src_df,\n",
    "            )\n",
    "\n",
    "        target_tbl = f\"hive_metastore.{get_catalog_name(self.entity_name,self.ingestion_layer)}.{self.entity_name.split('_inc')[0]}\"\n",
    "        return target_tbl\n",
    "\n",
    "    def execute_scd_2(self):\n",
    "        src_df, source_tbl = self.create_src_view()\n",
    "        # print(\"=======printing target before merge======\")\n",
    "        # display(spark.read.table(f\"hive_metastore.{get_catalog_name(self.entity_name,self.ingestion_layer)}.{self.entity_name.split('_inc')[0]}\"))\n",
    "        target_tbl = self.check_or_create_table(src_df)\n",
    "        pk_cond = \" AND \".join(\n",
    "            [f\"target.{pk_col} = source.{pk_col}\" for pk_col in self.pk]\n",
    "        )\n",
    "        update_cond = \" , \".join(\n",
    "            [f\"target.{u_col} = source.{u_col}\" for u_col in src_df.columns]\n",
    "        )\n",
    "        scd2_cond = \" OR \".join(\n",
    "            [\n",
    "                f\"target.{u_col} != source.{u_col}\"\n",
    "                for u_col in src_df.columns\n",
    "                if u_col not in [\"processing_dttm\"] + self.pk\n",
    "            ]\n",
    "        )\n",
    "        # print(update_cond)\n",
    "        insert_cols = \", \".join(\n",
    "            [i for i in src_df.columns if i not in (\"start_date\", \"end_date\")]\n",
    "        )\n",
    "        insert_values = \", \".join(\n",
    "            [\n",
    "                f\"{i}\"\n",
    "                for i in src_df.columns\n",
    "                if i not in (\"start_date\", \"end_date\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        query = f\"\"\"\n",
    "        MERGE INTO {target_tbl} AS target\n",
    "        USING {source_tbl} AS source\n",
    "        ON {pk_cond}\n",
    "        WHEN MATCHED AND ({scd2_cond}) THEN\n",
    "            UPDATE SET \n",
    "        target.end_date = CURRENT_TIMESTAMP\n",
    "    \n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT ({insert_cols}, target.start_date, target.end_date)\n",
    "                VALUES ({insert_values}, CURRENT_TIMESTAMP, NULL)\n",
    "        \"\"\"\n",
    "\n",
    "        spark.sql(query).display()\n",
    "        # display(spark.read.table(target_tbl))\n",
    "\n",
    "\n",
    "SCD2(\"Arancione_incremental\", \"ArancioneID\", ingestion_layer=\"silver\").execute_scd_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca807f5-9d6a-43a4-8456-464285891d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from hive_metastore.silver_incremental_schema.arancione"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5484845381312036,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD_Strategy",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
