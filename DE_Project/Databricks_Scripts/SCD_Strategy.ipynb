{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "773602bc-8442-4bdd-ba1c-54b19f510896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./CommonUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2795001-bccf-40be-94c0-f14351abaab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./CreateOrReplaceTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60f7f480-fa0c-4f80-bdd5-15b8abe0097e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faea656f-8152-4acf-bcc5-ede696ea2902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SCD1:\n",
    "    def __init__(self,entity_name,pk,ingestion_layer):\n",
    "        self.entity_name = entity_name.capitalize()\n",
    "        self.pk = pk.split(\",\")\n",
    "        self.ingestion_layer = ingestion_layer\n",
    "\n",
    "    def create_src_view(self):\n",
    "        src_df = spark.read.parquet(f\"{get_mnt('landing')}/{self.entity_name}/{self.entity_name}.parquet\")\n",
    "        src_df = src_df.withColumn(\"processing_dttm\",current_timestamp())\n",
    "        src_df.registerTempTable(\"source_tbl\")\n",
    "        return src_df,'source_tbl'\n",
    "    \n",
    "    def check_or_create_table(self,src_df):\n",
    "        table_exist_check = spark.sql(f\"show tables in {get_catalog_name(self.entity_name,self.ingestion_layer)}\").select('tableName').filter(f\"tableName = '{self.entity_name.split('_inc')[0]}'\").count() == 1\n",
    "        if table_exist_check:\n",
    "            print(\"we are going into crt_execute\")\n",
    "            print(f\"this is table_exist_check flag value: {table_exist_check} and its type is {type(table_exist_check)}\")\n",
    "            crt_execute(self.entity_name,self.ingestion_layer,truncate_flag=True,src_df=src_df)\n",
    "\n",
    "        target_tbl = f\"hive_metastore.{get_catalog_name(self.entity_name,self.ingestion_layer)}.{self.entity_name.split('_inc')[0]}\"\n",
    "        # print(\"======priting in check or create========\")\n",
    "        # display(spark.read.table(target_tbl))\n",
    "        return target_tbl\n",
    "    \n",
    "    def execute_scd_1(self):\n",
    "\n",
    "        src_df,source_tbl = self.create_src_view()\n",
    "        # print(\"=======printing target before merge======\")\n",
    "        # display(spark.read.table(f\"hive_metastore.{get_catalog_name(self.entity_name,self.ingestion_layer)}.{self.entity_name.split('_inc')[0]}\"))\n",
    "        target_tbl = self.check_or_create_table(src_df)\n",
    "        pk_cond = \" AND \".join([f\"target.{pk_col} = source.{pk_col}\" for pk_col in self.pk])\n",
    "        update_cond = \" , \".join([f\"target.{u_col} = source.{u_col}\" for u_col in src_df.columns])\n",
    "        # print(update_cond)\n",
    "        insert_cols= \", \".join(src_df.columns)\n",
    "        insert_values= \", \".join([f\"source.{icol}\" for icol in src_df.columns])\n",
    "        query = f'''\n",
    "        MERGE INTO {target_tbl} AS target\n",
    "        USING {source_tbl} AS source\n",
    "        ON {pk_cond}\n",
    "        WHEN MATCHED\n",
    "            THEN \n",
    "                UPDATE SET \n",
    "                    {update_cond}\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT ({insert_cols})\n",
    "            VALUES ({insert_values});\n",
    "\n",
    "        '''\n",
    "\n",
    "        spark.sql(query).display()\n",
    "        # display(spark.read.table(target_tbl))\n",
    "\n",
    "# SCD1('Celeste_incremental','TransactionId',ingestion_layer='bronze').execute_scd_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aec6dca-5d54-45f4-ae07-89b1b846ea3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SCD2:\n",
    "    def __init__(self,entity_name,pk,ingestion_layer):\n",
    "        self.entity_name = entity_name.capitalize()\n",
    "        self.pk = pk.split(\",\")\n",
    "        self.ingestion_layer = ingestion_layer\n",
    "\n",
    "    def create_src_view(self):\n",
    "        catalog_name = \"bronze_incremental_schema\" if self.entity_name.endswith('_incremental') else \"bronze_schema\"\n",
    "        src_df = spark.read.table(f\"{catalog_name}.{self.entity_name.split('_inc')[0]}\")\n",
    "        if self.entity_name.endswith('_incremental'):\n",
    "            src_df = src_df.withColumn(\"start_date\",current_timestamp())\n",
    "            src_df = src_df.withColumn(\"end_date\",current_timestamp())\n",
    "        else:\n",
    "            src_df = src_df.withColumn(\"processing_dttm\",current_timestamp())\n",
    "        src_df.registerTempTable(\"source_tbl\")\n",
    "        return src_df,'source_tbl'\n",
    "    \n",
    "    def check_or_create_table(self,src_df):\n",
    "        table_exist_check = spark.sql(f\"show tables in {get_catalog_name(self.entity_name,self.ingestion_layer)}\").select('tableName').filter(f\"tableName = '{self.entity_name.split('_inc')[0]}'\").count() == 1\n",
    "        if table_exist_check!=1:\n",
    "            crt_execute(self.entity_name,self.ingestion_layer,truncate_flag=True,src_df=src_df)\n",
    "\n",
    "        target_tbl = f\"hive_metastore.{get_catalog_name(self.entity_name,self.ingestion_layer)}.{self.entity_name.split('_inc')[0]}\"\n",
    "        return target_tbl\n",
    "    \n",
    "    def execute_scd_2(self):\n",
    "\n",
    "        src_df,source_tbl = self.create_src_view()\n",
    "        target_tbl = self.check_or_create_table(src_df)\n",
    "        pk_cond = \" AND \".join([f\"target.{pk_col} = source.{pk_col}\" for pk_col in self.pk])\n",
    "        update_cond = \" , \".join([f\"target.{u_col} = source.{u_col}\" for u_col in src_df.columns])\n",
    "        insert_cols= \", \".join(src_df.columns)\n",
    "        insert_values= \", \".join([f\"source.{icol}\" for icol in src_df.columns])\n",
    "        query = f'''\n",
    "        MERGE INTO {target_tbl} AS target\n",
    "        USING {source_tbl} AS source\n",
    "        ON {pk_cond}\n",
    "        WHEN MATCHED\n",
    "            THEN \n",
    "                UPDATE SET \n",
    "                    {update_cond}\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT ({insert_cols})\n",
    "            VALUES ({insert_values});\n",
    "\n",
    "        '''\n",
    "\n",
    "        spark.sql(query).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29f33d5b-0b6f-475b-8cd6-93733c686d90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ad7d86e-aa66-4092-8b8b-ea129b50c5d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'TransactionId'.split(',')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6812213841852458,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD_Strategy",
   "widgets": {
    "entity_name": {
     "currentValue": "Celeste_incremental",
     "nuid": "c6afb137-c154-4438-9f9c-f9558bfdaad4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "entity_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "entity_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "pk": {
     "currentValue": "TransactionId",
     "nuid": "2853563a-07eb-4dd6-9942-1aaca9a3b4ca",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "pk",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "pk",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
