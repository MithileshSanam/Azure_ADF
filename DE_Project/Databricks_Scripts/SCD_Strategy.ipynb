{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faea656f-8152-4acf-bcc5-ede696ea2902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SCD1:\n",
    "    def __init__(self,entity_name,pk,ingestion_layer):\n",
    "        self.entity_name = entity_name.capitalize()\n",
    "        self.pk = pk.split(\",\")\n",
    "        self.ingestion_layer = ingestion_layer\n",
    "\n",
    "    def create_src_view(self):\n",
    "        src_df = spark.read.parquet(f\"{get_mnt('landing')}/{self.entity_name}/{self.entity_name}.parquet\")\n",
    "        src_df = src_df.withColumn(\"processing_dttm\",current_timestamp())\n",
    "        src_df.registerTempTable(\"source_tbl\")\n",
    "        return src_df,'source_tbl'\n",
    "    \n",
    "    def check_or_create_table(self,src_df):\n",
    "        table_count = spark.sql(f\"show tables in {get_catalog_name(self.entity_name,self.ingestion_layer)}\").select('tableName').filter(f\"tableName = '{self.entity_name.lower().split('_inc')[0]}'\").count()\n",
    "        print(table_count)\n",
    "        if table_count ==0:\n",
    "            print(\"Table doesnt exist. Creating schema\")\n",
    "            crt_execute(self.entity_name,self.ingestion_layer,truncate_flag=True,src_df=src_df)\n",
    "\n",
    "        target_tbl = f\"hive_metastore.{get_catalog_name(self.entity_name,self.ingestion_layer)}.{self.entity_name.split('_inc')[0]}\"\n",
    "        return target_tbl\n",
    "    \n",
    "    def execute_scd_1(self):\n",
    "\n",
    "        src_df,source_tbl = self.create_src_view()\n",
    "        target_tbl = self.check_or_create_table(src_df)\n",
    "        pk_cond = \" AND \".join([f\"target.{pk_col} = source.{pk_col}\" for pk_col in self.pk])\n",
    "        update_cond = \" , \".join([f\"target.{u_col} = source.{u_col}\" for u_col in src_df.columns])\n",
    "        insert_cols= \", \".join(src_df.columns)\n",
    "        insert_values= \", \".join([f\"source.{icol}\" for icol in src_df.columns])\n",
    "        query = f'''\n",
    "        MERGE INTO {target_tbl} AS target\n",
    "        USING {source_tbl} AS source\n",
    "        ON {pk_cond}\n",
    "        WHEN MATCHED\n",
    "            THEN \n",
    "                UPDATE SET \n",
    "                    {update_cond}\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT ({insert_cols})\n",
    "            VALUES ({insert_values});\n",
    "\n",
    "        '''\n",
    "\n",
    "        spark.sql(query).display()\n",
    "# SCD1('Arancione_incremental','ArancioneId',ingestion_layer='bronze').execute_scd_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4461aca-21d2-46c8-962c-e5be8c907e07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SCD2:\n",
    "    def __init__(self, entity_name, pk, ingestion_layer):\n",
    "        self.entity_name = entity_name.capitalize()\n",
    "        self.pk = pk.split(\",\")\n",
    "        self.ingestion_layer = ingestion_layer\n",
    "\n",
    "    def table_check_count(self):\n",
    "        table_count = (\n",
    "            spark.sql(\n",
    "                f\"show tables in {get_catalog_name(self.entity_name,self.ingestion_layer)}\"\n",
    "            )\n",
    "            .select(\"tableName\")\n",
    "            .filter(f\"tableName = '{self.entity_name.lower().split('_inc')[0]}'\")\n",
    "            .count()\n",
    "        )\n",
    "        return table_count\n",
    "    \n",
    "    def check_or_create_table(self,src_df):\n",
    "        table_count = spark.sql(f\"show tables in {get_catalog_name(self.entity_name,self.ingestion_layer)}\").select('tableName').filter(f\"tableName = '{self.entity_name.lower().split('_inc')[0]}'\").count()\n",
    "        print(table_count)\n",
    "        if table_count ==0:\n",
    "            print(\"Table doesnt exist. Creating schema\")\n",
    "            src_df = src_df.withColumn(\"start_date\",lit(None).cast(TimestampType()))\n",
    "            src_df = src_df.withColumn(\"end_date\",lit(None).cast(TimestampType()))\n",
    "            src_df = src_df.withColumn(\"processing_dttm\",lit(None).cast(TimestampType()))\n",
    "            crt_execute(self.entity_name,self.ingestion_layer,truncate_flag=True,src_df=src_df)\n",
    "\n",
    "        target_tbl = f\"hive_metastore.{get_catalog_name(self.entity_name,self.ingestion_layer)}.{self.entity_name.split('_inc')[0]}\"\n",
    "        return target_tbl\n",
    "\n",
    "    def check_columns_presence(self, source_df, target_df, metadata_cols):\n",
    "        \"\"\"\n",
    "        Check if all columns from the target DataFrame are present in the source DataFrame.\n",
    "\n",
    "        Args:\n",
    "            source_df (pyspark.sql.DataFrame): Source DataFrame.\n",
    "            target_df (pyspark.sql.DataFrame): Target DataFrame.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If columns are missing in the source DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        cols_missing = set([cols for cols in target_df.columns if cols not in source_df.columns]) - set(metadata_cols)\n",
    "        if cols_missing:\n",
    "            raise Exception(f\"Cols missing in source DataFrame: {cols_missing}\")\n",
    "\n",
    "    def apply_hash_and_alias(self, source_df, target_df, metadata_cols) -> ([DataFrame, DataFrame]):\n",
    "        \"\"\"\n",
    "        Apply hash calculation and alias to source and target DataFrames.\n",
    "\n",
    "        Args:\n",
    "            source_df (pyspark.sql.DataFrame): Source DataFrame.\n",
    "            target_df (pyspark.sql.DataFrame): Target DataFrame.\n",
    "            metadata_cols (list): List of metadata columns to exclude from hash calculation.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing aliased source DataFrame and aliased target DataFrame.\n",
    "        \"\"\"\n",
    "        # Extract columns from target DataFrame excluding metadata columns\n",
    "        tgt_cols = [x for x in target_df.columns if x not in metadata_cols]\n",
    "\n",
    "        # Calculate hash expression\n",
    "        hash_expr = md5(concat_ws(\"|\", *[col(c) for c in tgt_cols]))\n",
    "\n",
    "        # Apply hash calculation and alias to source and target DataFrames\n",
    "        source_df = source_df.withColumn(\"hash_value\", hash_expr).alias(\"source_df\")\n",
    "        target_df = target_df.withColumn(\"hash_value\", hash_expr).alias(\"target_df\")\n",
    "\n",
    "        return source_df, target_df\n",
    "    \n",
    "    def create_src_df(self):\n",
    "        src_df = spark.read.table(\n",
    "            f\"hive_metastore.bronze_incremental_schema.{self.entity_name.lower().split('_inc')[0]}\")\n",
    "        if 'lastUpdateDate' in src_df.columns:\n",
    "            src_df=src_df.withColumnRenamed(\"lastUpdateDate\", \"sourceLastUpdateDate\")\n",
    "        if self.table_check_count() != 0:\n",
    "            max_ts = src_df.selectExpr(\"max(processing_dttm)\").collect()[0][0]\n",
    "            src_df = src_df.filter(f'processing_dttm = \"{max_ts}\"')\n",
    "        src_df = src_df.drop('processing_dttm')\n",
    "        return src_df\n",
    "\n",
    "\n",
    "    def scd_2(self, source_df, tgt_df, join_keys, metadata_cols=None) -> DataFrame:\n",
    "        if metadata_cols is None:\n",
    "            metadata_cols = ['start_date', 'end_date','processing_dttm']\n",
    "        tgt_cols = [x for x in tgt_df.columns]\n",
    "        self.check_columns_presence(source_df, tgt_df, metadata_cols)\n",
    "        # Apply hash calculation and alias\n",
    "\n",
    "        tgt_untouched_df = tgt_df.filter('end_date is not null')\n",
    "        target_df = tgt_df.filter('end_date is null')\n",
    "\n",
    "\n",
    "\n",
    "        source_df, target_df = self.apply_hash_and_alias(source_df, target_df, metadata_cols)\n",
    "\n",
    "        # Identify new records\n",
    "        join_cond = [source_df[join_key] == target_df[join_key] for join_key in join_keys]\n",
    "        new_df = source_df.join(target_df, join_cond, 'left_anti')\n",
    "\n",
    "        base_df = target_df.join(source_df, join_cond, 'left')\n",
    "\n",
    "\n",
    "        # Filter unchanged records or same records\n",
    "        unchanged_filter_expr = \" AND \".join([f\"source_df.{key} IS NULL\" for key in join_keys])\n",
    "        unchanged_df = base_df.filter(f\"({unchanged_filter_expr}) OR \"\n",
    "                                      f\"(source_df.hash_value = target_df.hash_value)\") \\\n",
    "            .select(\"target_df.*\")\n",
    "\n",
    "        # identify updated records\n",
    "        delta_filter_expr = \" and \".join([f\"source_df.{key} IS NOT NULL\" for key in join_keys])\n",
    "        updated_df = base_df.filter(f\"{delta_filter_expr} AND \"\n",
    "                                    f\"source_df.hash_value != target_df.hash_value\")\n",
    "\n",
    "\n",
    "        # pick updated records from source_df for new entry\n",
    "        updated_new_df = updated_df.select(\"source_df.*\")\n",
    "\n",
    "        # pick updated records from target_df for obsolete entry\n",
    "        obsolete_df = updated_df.select(\"target_df.*\") \\\n",
    "            .withColumn(\"end_date\", current_timestamp())\n",
    "\n",
    "        # union : new & updated records and add scd2 meta-deta\n",
    "        delta_df = new_df.union(updated_new_df) \\\n",
    "            .withColumn(\"start_date\", current_timestamp()) \\\n",
    "            .withColumn(\"end_date\", lit(None)) \\\n",
    "            .withColumn(\"processing_dttm\", current_timestamp())\n",
    "\n",
    "        # union all datasets : delta_df + obsolete_df + unchanged_df\n",
    "        result_df = unchanged_df.select(tgt_cols). \\\n",
    "            unionByName(delta_df.select(tgt_cols)). \\\n",
    "            unionByName(obsolete_df.select(tgt_cols)). \\\n",
    "            unionByName(tgt_untouched_df.select(tgt_cols))\n",
    "  \n",
    "        return result_df\n",
    "    \n",
    "    def execute_scd_2(self):\n",
    "        source_df = self.create_src_df()\n",
    "        silver_table_name = self.check_or_create_table(source_df)\n",
    "        # silver_table_name = f\"hive_metastore.{get_catalog_name(self.entity_name,self.ingestion_layer)}.{self.entity_name.split('_inc')[0]}\"\n",
    "\n",
    "        target_df = spark.read.table(silver_table_name)\n",
    "        scd2_df= self.scd_2(source_df, target_df, join_keys=self.pk, metadata_cols=None)\n",
    "\n",
    "        source_tbl='scd2_table'\n",
    "        scd2_df.registerTempTable(source_tbl)\n",
    "\n",
    "        pk = self.pk+['start_date']\n",
    "        pk_cond = \" AND \".join([f\"target.{pk_col} = source.{pk_col}\" for pk_col in pk])\n",
    "\n",
    "        insert_cols= \", \".join(scd2_df.columns)\n",
    "        insert_values= \", \".join([f\"source.{icol}\" for icol in scd2_df.columns])\n",
    "\n",
    "        query = f'''\n",
    "        MERGE INTO {silver_table_name} AS target\n",
    "        USING {source_tbl} AS source\n",
    "        ON {pk_cond}\n",
    "        WHEN MATCHED AND (source.end_date is not NULL and target.end_date is NULL)\n",
    "            THEN \n",
    "                UPDATE SET \n",
    "                    target.end_date = CURRENT_TIMESTAMP\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT ({insert_cols})\n",
    "            VALUES ({insert_values});\n",
    "\n",
    "        '''\n",
    "\n",
    "        spark.sql(query).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639ff272-9682-4f84-9c01-b0f056474a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# class SCD2:\n",
    "#     def __init__(self, entity_name, pk, ingestion_layer):\n",
    "#         self.entity_name = entity_name.capitalize()\n",
    "#         self.pk = pk.split(\",\")\n",
    "#         self.ingestion_layer = ingestion_layer\n",
    "\n",
    "#     def table_check_count(self):\n",
    "#         table_count = (\n",
    "#             spark.sql(\n",
    "#                 f\"show tables in {get_catalog_name(self.entity_name,self.ingestion_layer)}\"\n",
    "#             )\n",
    "#             .select(\"tableName\")\n",
    "#             .filter(f\"tableName = '{self.entity_name.lower().split('_inc')[0]}'\")\n",
    "#             .count()\n",
    "#         )\n",
    "#         return table_count\n",
    "\n",
    "#     def create_src_view(self):\n",
    "#         src_df = spark.read.table(\n",
    "#             f\"hive_metastore.bronze_incremental_schema.{self.entity_name.lower().split('_inc')[0]}\"\n",
    "#         ).withColumnRenamed(\"lastUpdateDate\", \"sourceLastUpdateDate\")\n",
    "#         if self.table_check_count() != 0:\n",
    "#             max_ts = src_df.selectExpr(\"max(processing_dttm)\").collect()[0][0]\n",
    "#             src_df = src_df.filter(f'processing_dttm = \"{max_ts}\"')\n",
    "#         src_df = src_df.drop('processing_dttm')\n",
    "#         return src_df\n",
    "\n",
    "#     def check_or_create_table(self, src_df):\n",
    "#         if self.table_check_count() == 0:\n",
    "#             print(\"Table doesnt exist. Creating schema\")\n",
    "#             crt_execute(\n",
    "#                 self.entity_name,\n",
    "#                 self.ingestion_layer,\n",
    "#                 truncate_flag=True,\n",
    "#                 src_df=src_df,\n",
    "#             )\n",
    "\n",
    "#         target_tbl = f\"hive_metastore.{get_catalog_name(self.entity_name,self.ingestion_layer)}.{self.entity_name.split('_inc')[0]}\"\n",
    "#         return target_tbl\n",
    "\n",
    "#     def execute_scd_2(self):\n",
    "#         src_df, source_tbl = self.create_src_view()\n",
    "#         # print(\"=======printing target before merge======\")\n",
    "#         # display(spark.read.table(f\"hive_metastore.{get_catalog_name(self.entity_name,self.ingestion_layer)}.{self.entity_name.split('_inc')[0]}\"))\n",
    "#         target_tbl = self.check_or_create_table(src_df)\n",
    "#         pk_cond = \" AND \".join(\n",
    "#             [f\"target.{pk_col} = source.{pk_col}\" for pk_col in self.pk]\n",
    "#         )\n",
    "#         update_cond = \" , \".join(\n",
    "#             [f\"target.{u_col} = source.{u_col}\" for u_col in src_df.columns]\n",
    "#         )\n",
    "#         scd2_cond = \" OR \".join(\n",
    "#             [\n",
    "#                 f\"target.{u_col} != source.{u_col}\"\n",
    "#                 for u_col in src_df.columns\n",
    "#                 if u_col not in [\"processing_dttm\",\"start_date\",'end_date'] + self.pk\n",
    "#             ]\n",
    "#         )\n",
    "#         # print(update_cond)\n",
    "#         insert_cols = \", \".join(\n",
    "#             [i for i in src_df.columns if i not in (\"start_date\", \"end_date\")]\n",
    "#         )\n",
    "#         insert_values = \", \".join(\n",
    "#             [\n",
    "#                 f\"{i}\"\n",
    "#                 for i in src_df.columns\n",
    "#                 if i not in (\"start_date\", \"end_date\")\n",
    "#             ]\n",
    "#         )\n",
    "\n",
    "#         query = f\"\"\"\n",
    "#         MERGE INTO {target_tbl} AS target\n",
    "#         USING {source_tbl} AS source\n",
    "#         ON {pk_cond}\n",
    "#         WHEN MATCHED AND (target.end_date is NULL) AND ({scd2_cond}) THEN\n",
    "#             UPDATE SET \n",
    "#         target.end_date = CURRENT_TIMESTAMP\n",
    "    \n",
    "#         WHEN NOT MATCHED THEN\n",
    "#             INSERT ({insert_cols}, target.start_date, target.end_date)\n",
    "#                 VALUES ({insert_values}, CURRENT_TIMESTAMP, NULL)\n",
    "#         \"\"\"\n",
    "\n",
    "#         spark.sql(query).display()\n",
    "\n",
    "#         tgt_df = spark.read.table(f\"hive_metastore.silver_incremental_schema.{self.entity_name.split('_inc')[0]}\").filter('end_date is null')\n",
    "\n",
    "#         tgt_df.registerTempTable('target_table')\n",
    "#         src_df.registerTempTable('source_table')\n",
    "\n",
    "#         ingest_df = spark.sql(\n",
    "#             f'''\n",
    "#             select source.* from source_table source inner join target_table target on {pk_cond} and\n",
    "#             ({scd2_cond})\n",
    "#             '''\n",
    "#         )\n",
    "#         ingest_df = ingest_df.withColumn('start_date',current_timestamp())\n",
    "#         ingest_df = ingest_df.union(tgt_df).distinct()\n",
    "#         ingest_df.write.mode(\"append\").saveAsTable(f\"hive_metastore.silver_incremental_schema.{self.entity_name.split('_inc')[0]}\")\n",
    "\n",
    "# # SCD2(\"Arancione_incremental\", \"ArancioneID\", ingestion_layer=\"silver\").execute_scd_2()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7529437511344680,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD_Strategy",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
